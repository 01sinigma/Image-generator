"""
–ú–æ–¥—É–ª—å –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã –∏ —Ñ—É–Ω–∫—Ü–∏–π
"""

import torch
import logging
from typing import Dict, List, Tuple
from pathlib import Path

logger = logging.getLogger(__name__)


class SystemStatusChecker:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ —Å–∏—Å—Ç–µ–º—ã –∏ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Ñ—É–Ω–∫—Ü–∏–π"""
    
    def __init__(self):
        self.status = {}
        self.functions_status = {}
    
    def check_system(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ —Å–∏—Å—Ç–µ–º–Ω—ã—Ö –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤"""
        status = {
            "cuda": self._check_cuda(),
            "gpu": self._check_gpu(),
            "models": self._check_models(),
            "config": self._check_config(),
            "dependencies": self._check_dependencies()
        }
        self.status = status
        return status
    
    def check_functions(self, generators: dict) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ —Ñ—É–Ω–∫—Ü–∏–π –¥–ª—è –∑–∞–≥—Ä—É–∂–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        functions = {}
        
        for model_type, generator in generators.items():
            model_functions = {
                "generate": hasattr(generator, 'generate') and callable(getattr(generator, 'generate', None)),
                "edit": hasattr(generator, 'edit') and callable(getattr(generator, 'edit', None)),
                "loaded": generator is not None and hasattr(generator, 'pipe') and generator.pipe is not None
            }
            functions[model_type] = model_functions
        
        self.functions_status = functions
        return functions
    
    def _check_cuda(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA"""
        try:
            available = torch.cuda.is_available()
            if available:
                device_count = torch.cuda.device_count()
                current_device = torch.cuda.current_device()
                device_name = torch.cuda.get_device_name(current_device)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏
                memory_total = torch.cuda.get_device_properties(current_device).total_memory / (1024**3)
                memory_allocated = torch.cuda.memory_allocated(current_device) / (1024**3)
                memory_reserved = torch.cuda.memory_reserved(current_device) / (1024**3)
                
                return {
                    "available": True,
                    "status": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç",
                    "device_count": device_count,
                    "current_device": current_device,
                    "device_name": device_name,
                    "memory_total_gb": round(memory_total, 2),
                    "memory_allocated_gb": round(memory_allocated, 2),
                    "memory_reserved_gb": round(memory_reserved, 2),
                    "memory_free_gb": round(memory_total - memory_reserved, 2),
                    "message": f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {device_name} ({memory_total:.1f}GB VRAM)"
                }
            else:
                return {
                    "available": False,
                    "status": "‚ùå –ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞",
                    "message": "CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω CPU"
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA: {e}")
            return {
                "available": False,
                "status": "‚ùå –û—à–∏–±–∫–∞",
                "message": f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ CUDA: {e}"
            }
    
    def _check_gpu(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ GPU —á–µ—Ä–µ–∑ nvidia-smi"""
        try:
            import subprocess
            result = subprocess.run(
                ['nvidia-smi', '--query-gpu=name,memory.total,memory.used,utilization.gpu', 
                 '--format=csv,noheader,nounits'],
                capture_output=True,
                text=True,
                timeout=3
            )
            
            if result.returncode == 0:
                lines = result.stdout.strip().split('\n')
                if lines:
                    parts = lines[0].split(', ')
                    return {
                        "available": True,
                        "status": "‚úÖ –†–∞–±–æ—Ç–∞–µ—Ç",
                        "name": parts[0],
                        "memory_total_mb": parts[1],
                        "memory_used_mb": parts[2],
                        "utilization_percent": parts[3],
                        "message": f"GPU: {parts[0]}, –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {parts[3]}%"
                    }
            
            return {
                "available": False,
                "status": "‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "message": "nvidia-smi –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω"
            }
        except FileNotFoundError:
            return {
                "available": False,
                "status": "‚ö†Ô∏è –ù–µ–¥–æ—Å—Ç—É–ø–µ–Ω",
                "message": "nvidia-smi –Ω–µ –Ω–∞–π–¥–µ–Ω (–≤–æ–∑–º–æ–∂–Ω–æ, –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã –¥—Ä–∞–π–≤–µ—Ä—ã NVIDIA)"
            }
        except Exception as e:
            logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU —á–µ—Ä–µ–∑ nvidia-smi: {e}")
            return {
                "available": None,
                "status": "‚ùì –ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ",
                "message": "–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å GPU"
            }
    
    def _check_models(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–µ–π"""
        try:
            from models import ModelFactory
            available_models = ModelFactory.get_available_models()
            return {
                "available": True,
                "status": "‚úÖ –î–æ—Å—Ç—É–ø–Ω—ã",
                "models": available_models,
                "count": len(available_models),
                "message": f"–î–æ—Å—Ç—É–ø–Ω–æ –º–æ–¥–µ–ª–µ–π: {len(available_models)} ({', '.join(available_models)})"
            }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}")
            return {
                "available": False,
                "status": "‚ùå –û—à–∏–±–∫–∞",
                "message": f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –º–æ–¥–µ–ª–µ–π: {e}"
            }
    
    def _check_config(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏"""
        try:
            import yaml
            config_path = Path("config.yaml")
            if config_path.exists():
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f)
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–ª—é—á–µ–≤—ã—Ö –Ω–∞—Å—Ç—Ä–æ–µ–∫
                device_config = config.get('device', {})
                enable_cpu_offload = device_config.get('enable_cpu_offload', False)
                sequential_offload = device_config.get('sequential_offload', False)
                
                issues = []
                if not enable_cpu_offload:
                    issues.append("CPU offload –æ—Ç–∫–ª—é—á–µ–Ω (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è –¥–ª—è 8GB VRAM)")
                
                return {
                    "available": True,
                    "status": "‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–∞",
                    "cpu_offload": enable_cpu_offload,
                    "sequential_offload": sequential_offload,
                    "issues": issues,
                    "message": f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∑–∞–≥—Ä—É–∂–µ–Ω–∞" + (f" ({len(issues)} –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–π)" if issues else "")
                }
            else:
                return {
                    "available": False,
                    "status": "‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω–∞",
                    "message": "config.yaml –Ω–µ –Ω–∞–π–¥–µ–Ω"
                }
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            return {
                "available": False,
                "status": "‚ùå –û—à–∏–±–∫–∞",
                "message": f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}"
            }
    
    def _check_dependencies(self) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π"""
        dependencies = {
            "torch": self._check_module("torch"),
            "diffusers": self._check_module("diffusers"),
            "transformers": self._check_module("transformers"),
            "gradio": self._check_module("gradio"),
            "pillow": self._check_module("PIL"),
            "yaml": self._check_module("yaml"),
        }
        
        available_count = sum(1 for dep in dependencies.values() if dep["available"])
        total_count = len(dependencies)
        
        return {
            "dependencies": dependencies,
            "available_count": available_count,
            "total_count": total_count,
            "status": "‚úÖ –í—Å–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã" if available_count == total_count else f"‚ö†Ô∏è {available_count}/{total_count}",
            "message": f"–ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {available_count}/{total_count} —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω—ã"
        }
    
    def _check_module(self, module_name: str) -> Dict:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è –º–æ–¥—É–ª—è"""
        try:
            if module_name == "PIL":
                import PIL
                version = PIL.__version__
            elif module_name == "yaml":
                import yaml
                version = getattr(yaml, '__version__', 'unknown')
            else:
                module = __import__(module_name)
                version = getattr(module, '__version__', 'unknown')
            
            return {
                "available": True,
                "status": "‚úÖ –£—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω",
                "version": version,
                "message": f"{module_name} {version}"
            }
        except ImportError:
            return {
                "available": False,
                "status": "‚ùå –ù–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω",
                "version": None,
                "message": f"{module_name} –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω"
            }
        except Exception as e:
            return {
                "available": False,
                "status": "‚ùå –û—à–∏–±–∫–∞",
                "version": None,
                "message": f"–û—à–∏–±–∫–∞ –ø—Ä–æ–≤–µ—Ä–∫–∏ {module_name}: {e}"
            }
    
    def get_status_summary(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å —Ç–µ–∫—Å—Ç–æ–≤—É—é —Å–≤–æ–¥–∫—É —Å—Ç–∞—Ç—É—Å–∞"""
        lines = []
        lines.append("üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã:")
        lines.append("")
        
        if "cuda" in self.status:
            cuda = self.status["cuda"]
            lines.append(f"  CUDA: {cuda.get('status', '‚ùì')} - {cuda.get('message', '')}")
        
        if "gpu" in self.status:
            gpu = self.status["gpu"]
            lines.append(f"  GPU: {gpu.get('status', '‚ùì')} - {gpu.get('message', '')}")
        
        if "models" in self.status:
            models = self.status["models"]
            lines.append(f"  –ú–æ–¥–µ–ª–∏: {models.get('status', '‚ùì')} - {models.get('message', '')}")
        
        if "config" in self.status:
            config = self.status["config"]
            lines.append(f"  –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è: {config.get('status', '‚ùì')} - {config.get('message', '')}")
        
        if "dependencies" in self.status:
            deps = self.status["dependencies"]
            lines.append(f"  –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏: {deps.get('status', '‚ùì')} - {deps.get('message', '')}")
        
        lines.append("")
        lines.append("üîß –§—É–Ω–∫—Ü–∏–∏:")
        
        for model_type, functions in self.functions_status.items():
            lines.append(f"  {model_type}:")
            for func_name, available in functions.items():
                status = "‚úÖ" if available else "‚ùå"
                lines.append(f"    {status} {func_name}: {'–î–æ—Å—Ç—É–ø–Ω–∞' if available else '–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞'}")
        
        return "\n".join(lines)
    
    def get_status_html(self) -> str:
        """–ü–æ–ª—É—á–∏—Ç—å HTML –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —Å—Ç–∞—Ç—É—Å–∞"""
        html_parts = []
        html_parts.append("<div style='font-family: monospace; font-size: 12px;'>")
        html_parts.append("<h3>üìä –°—Ç–∞—Ç—É—Å —Å–∏—Å—Ç–µ–º—ã</h3>")
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        for component_name, component_status in self.status.items():
            status_icon = component_status.get('status', '‚ùì')
            message = component_status.get('message', '')
            html_parts.append(f"<p><strong>{component_name.upper()}:</strong> {status_icon} {message}</p>")
        
        # –§—É–Ω–∫—Ü–∏–∏
        html_parts.append("<h3>üîß –§—É–Ω–∫—Ü–∏–∏ –º–æ–¥–µ–ª–µ–π</h3>")
        for model_type, functions in self.functions_status.items():
            html_parts.append(f"<p><strong>{model_type}:</strong></p>")
            html_parts.append("<ul>")
            for func_name, available in functions.items():
                status_icon = "‚úÖ" if available else "‚ùå"
                status_text = "–î–æ—Å—Ç—É–ø–Ω–∞" if available else "–ù–µ–¥–æ—Å—Ç—É–ø–Ω–∞"
                html_parts.append(f"<li>{status_icon} <code>{func_name}</code>: {status_text}</li>")
            html_parts.append("</ul>")
        
        html_parts.append("</div>")
        return "".join(html_parts)

