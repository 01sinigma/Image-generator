"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∞–±–æ—Ç—ã Qwen-Image-Edit-2511 –Ω–∞ 8GB VRAM
–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏, —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ—Å—Ç—å —Ä–∞–±–æ—Ç—ã –∏ –ø—Ä–∞–≤–∏–ª—å–Ω–æ—Å—Ç—å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
"""

import os
import sys
import time
import torch
import logging
from pathlib import Path
from PIL import Image
import yaml

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
try:
    from models import ModelFactory
except ImportError:
    logger.error("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å ModelFactory. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ models.py –¥–æ—Å—Ç—É–ø–µ–Ω.")
    sys.exit(1)


def get_gpu_memory_info():
    """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ–± –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ GPU –ø–∞–º—è—Ç–∏"""
    if not torch.cuda.is_available():
        return None
    
    device = torch.cuda.current_device()
    memory_allocated = torch.cuda.memory_allocated(device) / (1024**3)  # GB
    memory_reserved = torch.cuda.memory_reserved(device) / (1024**3)  # GB
    memory_total = torch.cuda.get_device_properties(device).total_memory / (1024**3)  # GB
    memory_free = memory_total - memory_reserved
    
    return {
        "allocated": memory_allocated,
        "reserved": memory_reserved,
        "total": memory_total,
        "free": memory_free,
        "usage_percent": (memory_reserved / memory_total) * 100
    }


def print_memory_info(label: str, gpu_info: dict = None):
    """–í—ã–≤–æ–¥ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–∞–º—è—Ç–∏"""
    print(f"\n{'='*60}")
    print(f"{label}")
    print(f"{'='*60}")
    
    if gpu_info:
        print(f"GPU Memory:")
        print(f"  –í—ã–¥–µ–ª–µ–Ω–æ:     {gpu_info['allocated']:.2f} GB")
        print(f"  –ó–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ: {gpu_info['reserved']:.2f} GB")
        print(f"  –í—Å–µ–≥–æ:        {gpu_info['total']:.2f} GB")
        print(f"  –°–≤–æ–±–æ–¥–Ω–æ:     {gpu_info['free']:.2f} GB")
        print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {gpu_info['usage_percent']:.1f}%")
    
    # –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ RAM
    try:
        import psutil
        ram = psutil.virtual_memory()
        print(f"\nRAM:")
        print(f"  –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ: {ram.used / (1024**3):.2f} GB")
        print(f"  –í—Å–µ–≥–æ:        {ram.total / (1024**3):.2f} GB")
        print(f"  –ü—Ä–æ—Ü–µ–Ω—Ç:      {ram.percent:.1f}%")
    except ImportError:
        print("\nRAM: psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω, –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")


def check_config():
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –¥–ª—è 8GB VRAM"""
    print("\n" + "="*60)
    print("–ü–†–û–í–ï–†–ö–ê –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ò –î–õ–Ø 8GB VRAM")
    print("="*60)
    
    try:
        with open("config.yaml", 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
    except Exception as e:
        logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å config.yaml: {e}")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
    device_config = config.get('device', {})
    enable_cpu_offload = device_config.get('enable_cpu_offload', False)
    sequential_offload = device_config.get('sequential_offload', False)
    
    print(f"\n–ù–∞—Å—Ç—Ä–æ–π–∫–∏ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞:")
    print(f"  enable_cpu_offload: {'‚úÖ –í–ö–õ–Æ–ß–ï–ù–û' if enable_cpu_offload else '‚ùå –í–´–ö–õ–Æ–ß–ï–ù–û'}")
    print(f"  sequential_offload: {'‚úÖ –í–ö–õ–Æ–ß–ï–ù–û' if sequential_offload else '‚ùå –í–´–ö–õ–Æ–ß–ï–ù–û'}")
    
    if not enable_cpu_offload:
        print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: enable_cpu_offload –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å –í–ö–õ–Æ–ß–ï–ù –¥–ª—è 8GB VRAM!")
        print("   –ë–µ–∑ —ç—Ç–æ–≥–æ –º–æ–¥–µ–ª—å –Ω–µ –ø–æ–º–µ—Å—Ç–∏—Ç—Å—è –≤ –ø–∞–º—è—Ç—å.")
        return False
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞—Å—Ç—Ä–æ–µ–∫ –º–æ–¥–µ–ª–∏ Qwen
    qwen_config = config.get('models', {}).get('qwen-image-edit', {})
    torch_dtype = qwen_config.get('torch_dtype', 'float16')
    low_cpu_mem_usage = qwen_config.get('low_cpu_mem_usage', False)
    
    print(f"\n–ù–∞—Å—Ç—Ä–æ–π–∫–∏ –º–æ–¥–µ–ª–∏ Qwen:")
    print(f"  torch_dtype: {'‚úÖ float16' if torch_dtype == 'float16' else f'‚ö†Ô∏è  {torch_dtype} (—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è float16)'}")
    print(f"  low_cpu_mem_usage: {'‚úÖ –í–ö–õ–Æ–ß–ï–ù–û' if low_cpu_mem_usage else '‚ùå –í–´–ö–õ–Æ–ß–ï–ù–û'}")
    
    if torch_dtype != 'float16':
        print("\n‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: torch_dtype –¥–æ–ª–∂–µ–Ω –±—ã—Ç—å 'float16' –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏!")
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π
    opt_config = config.get('optimization', {})
    compile_model = opt_config.get('compile_model', False)
    enable_flash_attention = opt_config.get('enable_flash_attention', False)
    
    print(f"\n–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏:")
    print(f"  compile_model: {'‚ùå –í–´–ö–õ–Æ–ß–ï–ù–û' if not compile_model else '‚ö†Ô∏è  –í–ö–õ–Æ–ß–ï–ù–û (–º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)'}")
    print(f"  enable_flash_attention: {'‚ùå –í–´–ö–õ–Æ–ß–ï–ù–û' if not enable_flash_attention else '‚ö†Ô∏è  –í–ö–õ–Æ–ß–ï–ù–û (–º–æ–∂–µ—Ç –∑–∞–Ω–∏–º–∞—Ç—å –±–æ–ª—å—à–µ –ø–∞–º—è—Ç–∏)'}")
    
    print("\n‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞")
    return True


def test_model_loading():
    """–¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏"""
    print("\n" + "="*60)
    print("–¢–ï–°–¢ –ó–ê–ì–†–£–ó–ö–ò –ú–û–î–ï–õ–ò")
    print("="*60)
    
    # –û—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    initial_memory = get_gpu_memory_info()
    print_memory_info("–ü–∞–º—è—Ç—å –î–û –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏", initial_memory)
    
    try:
        logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen-Image-Edit-2511...")
        start_time = time.time()
        
        generator = ModelFactory.create_generator('qwen-image-edit')
        
        load_time = time.time() - start_time
        logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∑–∞ {load_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        # –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
        loaded_memory = get_gpu_memory_info()
        print_memory_info("–ü–∞–º—è—Ç—å –ü–û–°–õ–ï –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏", loaded_memory)
        
        if loaded_memory:
            memory_used = loaded_memory['reserved'] - initial_memory['reserved']
            print(f"\nüìä –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ –ø–∞–º—è—Ç–∏ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ: {memory_used:.2f} GB")
            
            if loaded_memory['usage_percent'] > 95:
                print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –±–ª–∏–∑–∫–æ –∫ –º–∞–∫—Å–∏–º—É–º—É!")
            elif loaded_memory['usage_percent'] > 85:
                print("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
            else:
                print("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ")
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ CPU offload —Ä–∞–±–æ—Ç–∞–µ—Ç
        if hasattr(generator, 'pipe'):
            pipe = generator.pipe
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–µ –≤—Å–µ –Ω–∞ GPU
            components_on_gpu = 0
            total_components = 0
            
            for name, component in pipe.components.items():
                total_components += 1
                if hasattr(component, 'device'):
                    if str(component.device).startswith('cuda'):
                        components_on_gpu += 1
            
            print(f"\nüì¶ –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –º–æ–¥–µ–ª–∏:")
            print(f"  –í—Å–µ–≥–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤: {total_components}")
            print(f"  –ù–∞ GPU: {components_on_gpu}")
            print(f"  –ù–∞ CPU (offload): {total_components - components_on_gpu}")
            
            if components_on_gpu < total_components:
                print("‚úÖ CPU offload —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ - —á–∞—Å—Ç—å –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –Ω–∞ CPU")
            else:
                print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –í—Å–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –Ω–∞ GPU, CPU offload –º–æ–∂–µ—Ç –Ω–µ —Ä–∞–±–æ—Ç–∞—Ç—å!")
        
        return generator
        
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
        import traceback
        traceback.print_exc()
        return None


def test_image_editing(generator, test_image_path: str = None):
    """–¢–µ—Å—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    print("\n" + "="*60)
    print("–¢–ï–°–¢ –†–ï–î–ê–ö–¢–ò–†–û–í–ê–ù–ò–Ø –ò–ó–û–ë–†–ê–ñ–ï–ù–ò–Ø")
    print("="*60)
    
    if generator is None:
        print("‚ùå –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞—é —Ç–µ—Å—Ç")
        return False
    
    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ, –µ—Å–ª–∏ –Ω–µ —É–∫–∞–∑–∞–Ω–æ
    if test_image_path and os.path.exists(test_image_path):
        image = Image.open(test_image_path)
        print(f"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {test_image_path}")
    else:
        # –°–æ–∑–¥–∞–µ–º –ø—Ä–æ—Å—Ç–æ–µ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        image = Image.new('RGB', (512, 512), color='red')
        print("–°–æ–∑–¥–∞–Ω–æ —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ 512x512")
    
    prompt = "Add a blue sky in the background"
    
    # –ü–∞–º—è—Ç—å –ø–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
    before_memory = get_gpu_memory_info()
    print_memory_info("–ü–∞–º—è—Ç—å –ü–ï–†–ï–î —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º", before_memory)
    
    try:
        logger.info("–ù–∞—á–∞–ª–æ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...")
        start_time = time.time()
        
        # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU –≤–æ –≤—Ä–µ–º—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        if torch.cuda.is_available():
            # –ó–∞–ø—É—Å–∫–∞–µ–º –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø–æ—Ç–æ–∫–µ
            import threading
            monitoring = True
            
            def monitor_gpu():
                while monitoring:
                    try:
                        import subprocess
                        result = subprocess.run(
                            ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used,memory.total', 
                             '--format=csv,noheader,nounits'],
                            capture_output=True,
                            text=True,
                            timeout=2
                        )
                        if result.returncode == 0:
                            gpu_util, mem_used, mem_total = result.stdout.strip().split(', ')
                            print(f"  GPU: {gpu_util}% | VRAM: {mem_used}/{mem_total} MB")
                    except:
                        pass
                    time.sleep(2)
            
            monitor_thread = threading.Thread(target=monitor_gpu, daemon=True)
            monitor_thread.start()
        
        # –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ
        result = generator.edit(
            images=[image],
            prompt=prompt,
            num_inference_steps=10,  # –ú–µ–Ω—å—à–µ —à–∞–≥–æ–≤ –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ —Ç–µ—Å—Ç–∞
            guidance_scale=1.0,
            true_cfg_scale=4.0
        )
        
        monitoring = False
        edit_time = time.time() - start_time
        
        # –ü–∞–º—è—Ç—å –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
        after_memory = get_gpu_memory_info()
        print_memory_info("–ü–∞–º—è—Ç—å –ü–û–°–õ–ï —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è", after_memory)
        
        print(f"\n‚è±Ô∏è  –í—Ä–µ–º—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {edit_time:.2f} —Å–µ–∫—É–Ω–¥")
        
        if after_memory and before_memory:
            peak_memory = after_memory['reserved'] - before_memory['reserved']
            print(f"üìä –ü–∏–∫–æ–≤–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {peak_memory:.2f} GB")
            
            if after_memory['usage_percent'] > 95:
                print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –±–ª–∏–∑–∫–æ –∫ –º–∞–∫—Å–∏–º—É–º—É –≤–æ –≤—Ä–µ–º—è —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è!")
            else:
                print("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        output_dir = Path("outputs")
        output_dir.mkdir(exist_ok=True)
        output_path = output_dir / f"test_qwen_{int(time.time())}.png"
        result.save(output_path)
        print(f"‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {output_path}")
        
        return True
        
    except torch.cuda.OutOfMemoryError as e:
        logger.error(f"‚ùå –û–®–ò–ë–ö–ê: –ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ VRAM: {e}")
        print("\nüí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:")
        print("  1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ enable_cpu_offload: true –≤ config.yaml")
        print("  2. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ sequential_offload: true")
        print("  3. –ó–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ GPU")
        print("  4. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å —Ä–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
        return False
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
        import traceback
        traceback.print_exc()
        return False


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è"""
    print("="*60)
    print("–¢–ï–°–¢–ò–†–û–í–ê–ù–ò–ï QWEN-IMAGE-EDIT-2511 –ù–ê 8GB VRAM")
    print("="*60)
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
    if not torch.cuda.is_available():
        print("\n‚ùå CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞! –¢–µ—Å—Ç —Ç—Ä–µ–±—É–µ—Ç GPU.")
        return
    
    device_name = torch.cuda.get_device_name(0)
    device_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)
    
    print(f"\nüéÆ GPU: {device_name}")
    print(f"üíæ VRAM: {device_memory:.2f} GB")
    
    if device_memory < 7.5:
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: VRAM –º–µ–Ω—å—à–µ 8GB, –≤–æ–∑–º–æ–∂–Ω—ã –ø—Ä–æ–±–ª–µ–º—ã!")
    elif device_memory > 8.5:
        print("‚ÑπÔ∏è  VRAM –±–æ–ª—å—à–µ 8GB, —Ç–µ—Å—Ç –≤—Å–µ —Ä–∞–≤–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏")
    
    # 1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
    if not check_config():
        print("\n‚ùå –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º –¥–ª—è 8GB VRAM")
        print("   –ò—Å–ø—Ä–∞–≤—å—Ç–µ config.yaml –∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —Ç–µ—Å—Ç —Å–Ω–æ–≤–∞")
        return
    
    # 2. –¢–µ—Å—Ç –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏
    generator = test_model_loading()
    
    if generator is None:
        print("\n‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ.")
        return
    
    # 3. –¢–µ—Å—Ç —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    success = test_image_editing(generator)
    
    # –ò—Ç–æ–≥–∏
    print("\n" + "="*60)
    print("–ò–¢–û–ì–ò –¢–ï–°–¢–ò–†–û–í–ê–ù–ò–Ø")
    print("="*60)
    
    if success:
        print("‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –ø—Ä–æ–π–¥–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ!")
        print("\n–ú–æ–¥–µ–ª—å Qwen-Image-Edit-2511 –ø—Ä–∞–≤–∏–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞ –¥–ª—è —Ä–∞–±–æ—Ç—ã –Ω–∞ 8GB VRAM")
    else:
        print("‚ùå –ù–µ–∫–æ—Ç–æ—Ä—ã–µ —Ç–µ—Å—Ç—ã –Ω–µ –ø—Ä–æ—à–ª–∏")
        print("\n–ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ª–æ–≥–∏ –≤—ã—à–µ –¥–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—Ä–æ–±–ª–µ–º")
    
    # –§–∏–Ω–∞–ª—å–Ω–∞—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø–∞–º—è—Ç–∏
    final_memory = get_gpu_memory_info()
    print_memory_info("–§–∏–Ω–∞–ª—å–Ω–æ–µ —Å–æ—Å—Ç–æ—è–Ω–∏–µ –ø–∞–º—è—Ç–∏", final_memory)


if __name__ == "__main__":
    main()

