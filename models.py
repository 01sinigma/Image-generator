"""
–ú–æ–¥—É–ª—å –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–∞–∑–ª–∏—á–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
–ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç Z-Image-Turbo –∏ Qwen-Image-Edit-2511
"""

import os
import torch
import yaml
from pathlib import Path
from typing import Optional, Union, List
from PIL import Image
import logging
import time

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# –ò–º–ø–æ—Ä—Ç –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ, –µ—Å–ª–∏ –º–æ–¥—É–ª—å –¥–æ—Å—Ç—É–ø–µ–Ω)
try:
    from monitor import ProcessMonitor, ProgressTracker
    MONITOR_AVAILABLE = True
except ImportError:
    MONITOR_AVAILABLE = False
    ProcessMonitor = None
    ProgressTracker = None

# –ò–º–ø–æ—Ä—Ç –ø—Ä–æ–≤–µ—Ä–∫–∏ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏
try:
    from model_checker import ModelCompletenessChecker
    MODEL_CHECKER_AVAILABLE = True
except ImportError:
    MODEL_CHECKER_AVAILABLE = False
    ModelCompletenessChecker = None


class BaseImageGenerator:
    """–ë–∞–∑–æ–≤—ã–π –∫–ª–∞—Å—Å –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π"""
    
    def __init__(self, config: dict, model_config: dict):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –±–∞–∑–æ–≤–æ–≥–æ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        
        Args:
            config: –û–±—â–∞—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
            model_config: –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏
        """
        self.config = config
        self.model_config = model_config
        self.device = self._get_device()
        self.pipe = None
    
    def _get_device(self) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –¥–ª—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π"""
        device_config = self.config.get('device', {})
        device_type = device_config.get('type', 'auto')
        
        if device_type == 'auto':
            if torch.cuda.is_available():
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ CUDA –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
                    torch.cuda.current_device()
                    device_name = torch.cuda.get_device_name(0)
                    logger.info(f"–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: CUDA –¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_name}")
                    return 'cuda'
                except Exception as e:
                    logger.warning(f"CUDA –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞, –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                    return 'cpu'
            elif hasattr(torch.backends, 'mps') and torch.backends.mps.is_available():
                return 'mps'
            else:
                logger.info("–ê–≤—Ç–æ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ: CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return 'cpu'
        elif device_type == 'cuda':
            if not torch.cuda.is_available():
                logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU")
                return 'cpu'
            else:
                try:
                    # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ CUDA –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç
                    torch.cuda.current_device()
                    device_name = torch.cuda.get_device_name(0)
                    logger.info(f"CUDA –≤—ã–±—Ä–∞–Ω–∞ —è–≤–Ω–æ. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device_name}")
                    return 'cuda'
                except Exception as e:
                    logger.warning(f"CUDA –≤—ã–±—Ä–∞–Ω–∞, –Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞: {e}. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è CPU.")
                    return 'cpu'
        else:
            return device_type
    
    def _get_torch_dtype(self) -> torch.dtype:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Ç–∏–ø–∞ –¥–∞–Ω–Ω—ã—Ö PyTorch"""
        dtype_str = self.model_config.get('torch_dtype', 'bfloat16')
        
        # –ù–∞ CPU –∏—Å–ø–æ–ª—å–∑—É–µ–º float32 (float16/bfloat16 –º–æ–≥—É—Ç –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å—Å—è)
        if self.device == 'cpu':
            return torch.float32
        
        if dtype_str == 'bfloat16':
            return torch.bfloat16
        elif dtype_str == 'float16':
            return torch.float16
        else:
            return torch.float32
    
    def _save_image(self, image: Image.Image, save_path: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
        output_dir = Path(save_path).parent
        output_dir.mkdir(parents=True, exist_ok=True)
        
        output_format = self.config.get('output', {}).get('format', 'png')
        quality = self.config.get('output', {}).get('quality', 95)
        
        if output_format.lower() == 'jpg' or output_format.lower() == 'jpeg':
            image.save(save_path, format='JPEG', quality=quality)
        else:
            image.save(save_path, format='PNG')
        
        logger.info(f"–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {save_path}")


class ZImageGenerator(BaseImageGenerator):
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è Z-Image-Turbo (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)"""
    
    def __init__(self, config: dict, model_config: dict):
        super().__init__(config, model_config)
        self._load_pipeline()
    
    def _load_pipeline(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Z-Image-Turbo"""
        from diffusers import ZImagePipeline
        
        # –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞
        monitor = None
        progress = None
        if MONITOR_AVAILABLE:
            monitor = ProcessMonitor(timeout=600.0)  # 10 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç
            monitor.start()
            progress = ProgressTracker(total_steps=5, description="–ó–∞–≥—Ä—É–∑–∫–∞ Z-Image-Turbo")
        
        try:
            logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Z-Image-Turbo...")
            
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
            if progress:
                progress.update(1, "–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA...")
            if monitor:
                monitor.update_activity()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            if self.device == 'cuda':
                if not torch.cuda.is_available():
                    logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU")
                    self.device = 'cpu'
                else:
                    logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
                    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                    torch.cuda.empty_cache()
            
            # –®–∞–≥ 2: –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏
            if progress:
                progress.update(2, "–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–æ–¥–µ–ª–∏...")
            if monitor:
                monitor.update_activity()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏
            local_path = self.model_config.get('local_path')
            model_name = local_path if local_path and os.path.exists(local_path) else self.model_config.get('name', 'Tongyi-MAI/Z-Image-Turbo')
            
            if local_path and os.path.exists(local_path):
                logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {local_path}")
            else:
                logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
            
            torch_dtype = self._get_torch_dtype()
            low_cpu_mem_usage = self.model_config.get('low_cpu_mem_usage', False)
            
            # –®–∞–≥ 3: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            if progress:
                progress.update(3, "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face...")
            if monitor:
                monitor.update_activity()
            # –°–æ–≥–ª–∞—Å–Ω–æ –¥–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏–∏ Z-Image-Turbo, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å torch_dtype –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ
            # –ü–æ–ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å torch_dtype, –µ—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏—Ç—Å—è - –∑–∞–≥—Ä—É–∑–∏–º –±–µ–∑ –Ω–µ–≥–æ
            try:
                self.pipe = ZImagePipeline.from_pretrained(
                    model_name,
                    torch_dtype=torch_dtype,
                    low_cpu_mem_usage=low_cpu_mem_usage,
                    resume_download=True,  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                )
                logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å torch_dtype={torch_dtype}")
            except TypeError:
                # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∑–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ torch_dtype
                self.pipe = ZImagePipeline.from_pretrained(
                    model_name,
                    low_cpu_mem_usage=low_cpu_mem_usage,
                    resume_download=True,  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
                )
                logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ torch_dtype (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π dtype)")
            
            if monitor:
                monitor.update_activity()
            
            # –®–∞–≥ 4: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU offload
            if progress:
                progress.update(4, "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU offload...")
            if monitor:
                monitor.update_activity()
            
            # CPU offload —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å CUDA, –Ω–∞ CPU –ø—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
            if self.config['device'].get('enable_cpu_offload', False) and self.device == 'cuda':
                try:
                    use_sequential = self.config['device'].get('sequential_offload', False)
                    
                    if use_sequential:
                        # Sequential CPU offload - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å)
                        if hasattr(self.pipe, 'enable_sequential_cpu_offload'):
                            self.pipe.enable_sequential_cpu_offload()
                            logger.info("Sequential CPU offload –≤–∫–ª—é—á–µ–Ω (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
                        else:
                            self.pipe.enable_model_cpu_offload()
                            logger.info("CPU offload –≤–∫–ª—é—á–µ–Ω (sequential –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
                    else:
                        # –û–±—ã—á–Ω—ã–π CPU offload - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (–±—ã—Å—Ç—Ä–µ–µ)
                        self.pipe.enable_model_cpu_offload()
                        logger.info("CPU offload –≤–∫–ª—é—á–µ–Ω (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ VRAM)")
                    
                    # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –≤–∫–ª—é—á–µ–Ω–∏—è CPU offload
                    if torch.cuda.is_available():
                        torch.cuda.synchronize()
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å CPU offload: {e}. –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –Ω–∞ GPU.")
                    device_obj = torch.device(self.device)
                    self.pipe.to(device_obj)
            else:
                device_obj = torch.device(self.device)
                self.pipe.to(device_obj)
            
            # –®–∞–≥ 5: –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
            if progress:
                progress.update(5, "–§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞...")
            if monitor:
                monitor.update_activity()
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
            if self.device == 'cuda' and torch.cuda.is_available():
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
                # –ü—Ä–∏ CPU offload –ø–∞–º—è—Ç—å –º–æ–∂–µ—Ç –Ω–µ –ø–æ–∫–∞–∑—ã–≤–∞—Ç—å—Å—è —á–µ—Ä–µ–∑ memory_allocated
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –æ–±—â—É—é –ø–∞–º—è—Ç—å GPU
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_free = memory_total - memory_reserved
                logger.info(f"VRAM –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB")
            
            logger.info(f"–ú–æ–¥–µ–ª—å Z-Image-Turbo –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
            self._apply_optimizations()
            
            if progress:
                progress.finish("–ú–æ–¥–µ–ª—å Z-Image-Turbo —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if self.device == 'cuda' and torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise
        finally:
            if monitor:
                monitor.stop()
    
    def _apply_optimizations(self):
        """–ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–π"""
        opt_config = self.config.get('optimization', {})
        
        if opt_config.get('enable_flash_attention', False):
            attention_backend = opt_config.get('attention_backend', 'flash')
            if hasattr(self.pipe, 'transformer'):
                try:
                    if attention_backend == 'flash':
                        self.pipe.transformer.set_attention_backend("flash")
                        logger.info("Flash Attention 2 –≤–∫–ª—é—á–µ–Ω")
                    elif attention_backend == '_flash_3':
                        self.pipe.transformer.set_attention_backend("_flash_3")
                        logger.info("Flash Attention 3 –≤–∫–ª—é—á–µ–Ω")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å attention backend: {e}")
        
        if opt_config.get('compile_model', False):
            if hasattr(self.pipe, 'transformer'):
                try:
                    self.pipe.transformer.compile()
                    logger.info("–ú–æ–¥–µ–ª—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞–Ω–∞")
                except Exception as e:
                    logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–∫–æ–º–ø–∏–ª–∏—Ä–æ–≤–∞—Ç—å –º–æ–¥–µ–ª—å: {e}")
    
    def generate(
        self,
        prompt: str,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Image.Image:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É"""
        if self.pipe is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        gen_config = self.config.get('generation', {})
        height = height or gen_config.get('default_height', 1024)
        width = width or gen_config.get('default_width', 1024)
        num_inference_steps = num_inference_steps or gen_config.get('default_num_inference_steps', 9)
        guidance_scale = guidance_scale if guidance_scale is not None else gen_config.get('default_guidance_scale', 0.0)
        seed = seed if seed is not None else gen_config.get('default_seed')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        if self.device == 'cuda':
            if not torch.cuda.is_available():
                raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞–∫ 'cuda'")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ GPU –Ω–µ –∑–∞–Ω—è—Ç
            try:
                torch.cuda.synchronize()
                logger.info(f"CUDA —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
            except Exception as e:
                logger.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ CUDA: {e}")
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
            torch.cuda.empty_cache()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_free = memory_total - memory_reserved
                logger.info(f"VRAM –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º
        if seed is not None:
            device_obj = torch.device(self.device)
            generator = torch.Generator(device_obj).manual_seed(seed)
        else:
            generator = None
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {prompt[:50]}...")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {height}x{width}, —à–∞–≥–æ–≤: {num_inference_steps}, device: {self.device}")
        
        try:
            result = self.pipe(
                prompt=prompt,
                height=height,
                width=width,
                num_inference_steps=num_inference_steps,
                guidance_scale=guidance_scale,
                generator=generator,
            )
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è CUDA –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.device == 'cuda':
                torch.cuda.synchronize()
                torch.cuda.empty_cache()
            
            image = result.images[0]
            
            if save_path:
                self._save_image(image, save_path)
            
            logger.info("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
            return image
            
        except RuntimeError as e:
            error_msg = str(e)
            if "CUDA" in error_msg or "cuda" in error_msg:
                logger.error(f"CUDA –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                if self.device == 'cuda':
                    # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∏ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
                    try:
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                    except:
                        pass
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise


class QwenImageEditGenerator(BaseImageGenerator):
    """–ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –¥–ª—è Qwen-Image-Edit-2511 (–≥–µ–Ω–µ—Ä–∞—Ü–∏—è –∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π)"""
    
    def __init__(self, config: dict, model_config: dict):
        super().__init__(config, model_config)
        self._load_pipeline()
    
    def generate(
        self,
        prompt: str,
        height: Optional[int] = None,
        width: Optional[int] = None,
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        seed: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Image.Image:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
        
        Args:
            prompt: –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            height: –í—ã—Å–æ—Ç–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            width: –®–∏—Ä–∏–Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            num_inference_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            guidance_scale: –ú–∞—Å—à—Ç–∞–± guidance
            seed: –°–ª—É—á–∞–π–Ω–æ–µ –∑–µ—Ä–Ω–æ –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
            save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è (–æ–ø—Ü–∏–æ–Ω–∞–ª—å–Ω–æ)
        
        Returns:
            PIL.Image: –°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        if self.pipe is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        gen_config = self.config.get('generation', {})
        edit_config = self.model_config.get('edit', {})
        
        height = height or gen_config.get('default_height', 512)
        width = width or gen_config.get('default_width', 512)
        num_inference_steps = num_inference_steps or edit_config.get('default_num_inference_steps', 40)
        guidance_scale = guidance_scale if guidance_scale is not None else edit_config.get('default_guidance_scale', 1.0)
        true_cfg_scale = edit_config.get('default_true_cfg_scale', 4.0)
        seed = seed if seed is not None else gen_config.get('default_seed')
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
        if self.device == 'cuda':
            if not torch.cuda.is_available():
                raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞–∫ 'cuda'")
            
            try:
                torch.cuda.synchronize()
                logger.info(f"CUDA —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
            except Exception as e:
                logger.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ CUDA: {e}")
            
            torch.cuda.empty_cache()
            
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_free = memory_total - memory_reserved
                logger.info(f"VRAM –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º
        if seed is not None:
            device_obj = torch.device(self.device)
            generator = torch.Generator(device_obj).manual_seed(seed)
        else:
            generator = None
        
        logger.info(f"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {prompt[:50]}...")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: {height}x{width}, —à–∞–≥–æ–≤: {num_inference_steps}, device: {self.device}")
        
        try:
            # Qwen –º–æ–∂–µ—Ç –≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è, –ø–µ—Ä–µ–¥–∞–≤ –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è image
            # –≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç –≥–µ–Ω–µ—Ä–∞—Ü–∏—é —Å –Ω—É–ª—è –±–µ–∑ –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            inputs = {
                "image": [],  # –ü—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ —Å –Ω—É–ª—è
                "prompt": prompt,
                "generator": generator,
                "true_cfg_scale": true_cfg_scale,
                "negative_prompt": " ",
                "num_inference_steps": num_inference_steps,
                "guidance_scale": guidance_scale,
                "num_images_per_prompt": 1,
            }
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–µ–π
            if self.device == 'cuda' and torch.cuda.is_available():
                memory_before = torch.cuda.memory_allocated(0) / 1024**3
                logger.debug(f"VRAM –ø–µ—Ä–µ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º: {memory_before:.2f}GB")
            
            with torch.inference_mode():
                output = self.pipe(**inputs)
                image = output.images[0]
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è CUDA –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
            if self.device == 'cuda':
                torch.cuda.synchronize()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                if torch.cuda.is_available():
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    memory_free = memory_total - memory_reserved
                    usage_percent = (memory_reserved / memory_total) * 100
                    
                    logger.info(f"VRAM –ø–æ—Å–ª–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                    logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {usage_percent:.1f}%")
                    
                    if usage_percent > 95:
                        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –±–ª–∏–∑–∫–æ –∫ –º–∞–∫—Å–∏–º—É–º—É!")
                    elif usage_percent > 85:
                        logger.info("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –≤—ã—Å–æ–∫–æ–µ, –Ω–æ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
                    else:
                        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ")
                
                torch.cuda.empty_cache()
            
            if save_path:
                self._save_image(image, save_path)
            
            logger.info("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–æ")
            return image
            
        except RuntimeError as e:
            error_msg = str(e)
            if "CUDA" in error_msg or "cuda" in error_msg:
                logger.error(f"CUDA –æ—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
                if self.device == 'cuda':
                    try:
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                    except:
                        pass
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    def _load_pipeline(self):
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ Qwen-Image-Edit-2511 (GGUF –∏–ª–∏ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–∞—è)"""
        from diffusers import QwenImageEditPlusPipeline
        
        # –ó–∞–ø—É—Å–∫ –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞ –ø—Ä–æ—Ü–µ—Å—Å–∞
        monitor = None
        progress = None
        if MONITOR_AVAILABLE:
            monitor = ProcessMonitor(timeout=900.0)  # 15 –º–∏–Ω—É—Ç —Ç–∞–π–º–∞—É—Ç –¥–ª—è –±–æ–ª—å—à–æ–π –º–æ–¥–µ–ª–∏
            monitor.start()
            progress = ProgressTracker(total_steps=6, description="–ó–∞–≥—Ä—É–∑–∫–∞ Qwen-Image-Edit-2511")
        
        try:
            # –®–∞–≥ 1: –ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA
            if progress:
                progress.update(1, "–ü—Ä–æ–≤–µ—Ä–∫–∞ CUDA...")
            if monitor:
                monitor.update_activity()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
            if self.device == 'cuda':
                if not torch.cuda.is_available():
                    logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –ø–µ—Ä–µ–∫–ª—é—á–∞—é—Å—å –Ω–∞ CPU")
                    self.device = 'cpu'
                else:
                    logger.info(f"CUDA –¥–æ—Å—Ç—É–ø–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
                    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–µ—Ä–µ–¥ –∑–∞–≥—Ä—É–∑–∫–æ–π
                    torch.cuda.empty_cache()
        
            # –®–∞–≥ 2: –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏ –∏ –¥–æ–∫–∞—á–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤
            if progress:
                progress.update(2, "–î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏...")
            if monitor:
                monitor.update_activity()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ª–æ–∫–∞–ª—å–Ω–æ–≥–æ –ø—É—Ç–∏ –∏ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏
            local_path = self.model_config.get('local_path')
            use_gguf = self.model_config.get('use_gguf', False)
            model_name = self.model_config.get('name', 'Qwen/Qwen-Image-Edit-2511')
            
            # –î–µ—Ç–∞–ª—å–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏
            use_local = False
            if MODEL_CHECKER_AVAILABLE and (local_path or True):  # –ü—Ä–æ–≤–µ—Ä—è–µ–º –≤—Å–µ–≥–¥–∞, –¥–∞–∂–µ –µ—Å–ª–∏ local_path –Ω–µ —É–∫–∞–∑–∞–Ω (–∫—ç—à HF)
                try:
                    checker = ModelCompletenessChecker(model_name, local_path)
                    is_complete, report = checker.check_model_completeness()
                    
                    if not is_complete:
                        logger.warning("–ú–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è, –Ω–∞—á–∏–Ω–∞—é –¥–æ–∫–∞—á–∫—É –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤...")
                        if monitor:
                            monitor.update_activity()
                        
                        # –î–æ–∫–∞—á–∏–≤–∞–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ —Ñ–∞–π–ª—ã
                        download_success = checker.download_missing_files(report, max_retries=3)
                        
                        if download_success:
                            # –ü–æ–≤—Ç–æ—Ä–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ—Å–ª–µ –¥–æ–∫–∞—á–∫–∏
                            is_complete, report = checker.check_model_completeness()
                            if is_complete:
                                logger.info("‚úÖ –ú–æ–¥–µ–ª—å —Ç–µ–ø–µ—Ä—å –ø–æ–ª–Ω–∞—è –ø–æ—Å–ª–µ –¥–æ–∫–∞—á–∫–∏")
                                use_local = local_path and os.path.exists(local_path)
                            else:
                                logger.warning("‚ö†Ô∏è –ü–æ—Å–ª–µ –¥–æ–∫–∞—á–∫–∏ –º–æ–¥–µ–ª—å –≤—Å–µ –µ—â–µ –Ω–µ–ø–æ–ª–Ω–∞—è, –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face")
                        else:
                            logger.warning("‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –¥–æ–∫–∞—á–∞—Ç—å –≤—Å–µ —Ñ–∞–π–ª—ã, –±—É–¥–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face")
                    else:
                        logger.info("‚úÖ –ú–æ–¥–µ–ª—å –ø–æ–ª–Ω–∞—è, –º–æ–∂–Ω–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–æ–∫–∞–ª—å–Ω–æ")
                        use_local = local_path and os.path.exists(local_path)
                        
                except Exception as e:
                    logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ –ø–æ–ª–Ω–æ—Ç—ã –º–æ–¥–µ–ª–∏: {e}")
                    logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞—é –∑–∞–≥—Ä—É–∑–∫—É –∏–∑ Hugging Face...")
            else:
                # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞, –µ—Å–ª–∏ ModelCompletenessChecker –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω
                if local_path and os.path.exists(local_path):
                    model_index = os.path.join(local_path, 'model_index.json')
                    if os.path.exists(model_index):
                        use_local = True
                        logger.info(f"–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ª–æ–∫–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏: {local_path}")
                    else:
                        logger.warning(f"–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è: –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç model_index.json")
                else:
                    logger.info("–õ–æ–∫–∞–ª—å–Ω—ã–π –ø—É—Ç—å –Ω–µ —É–∫–∞–∑–∞–Ω –∏–ª–∏ –Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")
            
            if not use_local:
                if use_gguf:
                    logger.info(f"–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
                else:
                    logger.info(f"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face: {model_name}")
                model_name = self.model_config.get('name', 'Qwen/Qwen-Image-Edit-2511')
            
            # –®–∞–≥ 3: –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ Hugging Face
            if progress:
                progress.update(3, "–ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ Hugging Face...")
            if monitor:
                monitor.update_activity()
            
            torch_dtype = self._get_torch_dtype()
            low_cpu_mem_usage = self.model_config.get('low_cpu_mem_usage', False)
            
            # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –º–æ–¥–µ–ª–∏ (–º–æ–∂–µ—Ç –Ω–µ —Å—Ä–∞–±–æ—Ç–∞—Ç—å, —Ç.–∫. diffusers –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç GGUF –Ω–∞–ø—Ä—è–º—É—é)
            if use_gguf:
                logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ GGUF –≤–µ—Ä—Å–∏–∏ –º–æ–¥–µ–ª–∏...")
                try:
                    # –ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∏—Ç—å —á–µ—Ä–µ–∑ diffusers (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç –¥–ª—è GGUF)
                    self.pipe = QwenImageEditPlusPipeline.from_pretrained(
                        model_name,
                        torch_dtype=torch_dtype,
                        low_cpu_mem_usage=low_cpu_mem_usage,
                    )
                    logger.info(f"‚úÖ GGUF –º–æ–¥–µ–ª—å —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å torch_dtype={torch_dtype}")
                except Exception as e:
                    logger.warning(f"‚ö†Ô∏è –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å GGUF –º–æ–¥–µ–ª—å —á–µ—Ä–µ–∑ diffusers: {e}")
                    logger.info("üîÑ –ü–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å Qwen/Qwen-Image-Edit-2511 —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏ –¥–ª—è 8GB VRAM...")
                    # Fallback –Ω–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—É—é –º–æ–¥–µ–ª—å —Å –º–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–º–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
                    model_name = "Qwen/Qwen-Image-Edit-2511"
                    use_gguf = False
                    # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
                    if torch_dtype != torch.float16:
                        logger.info("–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ float16 –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏ –Ω–∞ 8GB VRAM")
                        torch_dtype = torch.float16
            else:
                logger.info("–ó–∞–≥—Ä—É–∑–∫–∞ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–π –º–æ–¥–µ–ª–∏ Qwen-Image-Edit-2511...")
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫—ç—à–∞ Hugging Face –Ω–∞ –Ω–∞–ª–∏—á–∏–µ —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                try:
                    from huggingface_hub import scan_cache_dir
                    cache_info = scan_cache_dir()
                    # –ò—â–µ–º –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ
                    for repo in cache_info.repos:
                        if 'Qwen-Image-Edit-2511' in str(repo.repo_id) or 'qwen-image-edit' in str(repo.repo_id).lower():
                            logger.info(f"–ù–∞–π–¥–µ–Ω–∞ —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω–∞—è –º–æ–¥–µ–ª—å –≤ –∫—ç—à–µ: {repo.repo_id}")
                            logger.info(f"–†–∞–∑–º–µ—Ä –≤ –∫—ç—à–µ: {repo.size_on_disk_str}")
                            logger.info("–ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤...")
                            break
                except Exception as e:
                    logger.debug(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –∫—ç—à: {e}")
            
            # –®–∞–≥ 4: –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏
            if progress:
                progress.update(4, "–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ Hugging Face...")
            if monitor:
                monitor.update_activity()
            
            try:
                # QwenImageEditPlusPipeline –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–æ–¥–æ–ª–∂–∏—Ç –∑–∞–≥—Ä—É–∑–∫—É —á–∞—Å—Ç–∏—á–Ω–æ —Å–∫–∞—á–∞–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤
                # from_pretrained –∏—Å–ø–æ–ª—å–∑—É–µ—Ç resume_download=True –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                logger.info("–ü—Ä–æ–≤–µ—Ä–∫–∞ –∏ –¥–æ–∫–∞—á–∫–∞ –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ñ–∞–π–ª–æ–≤ –º–æ–¥–µ–ª–∏...")
                try:
                    self.pipe = QwenImageEditPlusPipeline.from_pretrained(
                        model_name,
                        torch_dtype=torch_dtype,
                        low_cpu_mem_usage=low_cpu_mem_usage,
                        resume_download=True,  # –Ø–≤–Ω–æ —É–∫–∞–∑—ã–≤–∞–µ–º –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ –∑–∞–≥—Ä—É–∑–∫–∏
                    )
                    logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ —Å torch_dtype={torch_dtype}")
                    if monitor:
                        monitor.update_activity()
                except TypeError:
                    # –ï—Å–ª–∏ –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è, –∑–∞–≥—Ä—É–∂–∞–µ–º –±–µ–∑ –Ω–µ–≥–æ
                    self.pipe = QwenImageEditPlusPipeline.from_pretrained(
                        model_name,
                        low_cpu_mem_usage=low_cpu_mem_usage,
                        resume_download=True,  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
                    )
                    logger.info("–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±–µ–∑ torch_dtype (–±—É–¥–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π dtype)")
                    if monitor:
                        monitor.update_activity()
                except FileNotFoundError as e:
                    # –ï—Å–ª–∏ –ª–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è, –ø—Ä–æ–±—É–µ–º –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑ Hugging Face
                    if use_local and local_path:
                        logger.warning(f"–õ–æ–∫–∞–ª—å–Ω–∞—è –º–æ–¥–µ–ª—å –Ω–µ–ø–æ–ª–Ω–∞—è: {e}")
                        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∏–∑ Hugging Face...")
                        model_name = self.model_config.get('name', 'Qwen/Qwen-Image-Edit-2511')
                        self.pipe = QwenImageEditPlusPipeline.from_pretrained(
                            model_name,
                            torch_dtype=torch_dtype,
                            low_cpu_mem_usage=low_cpu_mem_usage,
                            resume_download=True,  # –ü—Ä–æ–¥–æ–ª–∂–∞–µ–º –∑–∞–≥—Ä—É–∑–∫—É
                        )
                        logger.info(f"–ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ Hugging Face —Å torch_dtype={torch_dtype}")
                        if monitor:
                            monitor.update_activity()
                    else:
                        raise
                
                # –®–∞–≥ 5: –ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU offload
                if progress:
                    progress.update(5, "–ù–∞—Å—Ç—Ä–æ–π–∫–∞ CPU offload...")
                if monitor:
                    monitor.update_activity()
                
                # CPU offload —Ä–∞–±–æ—Ç–∞–µ—Ç —Ç–æ–ª—å–∫–æ —Å CUDA, –Ω–∞ CPU –ø—Ä–æ—Å—Ç–æ –∑–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
                if self.config['device'].get('enable_cpu_offload', False) and self.device == 'cuda':
                    try:
                        use_sequential = self.config['device'].get('sequential_offload', False)
                        
                        if use_sequential:
                            # Sequential CPU offload - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã –ø–æ –æ–¥–Ω–æ–º—É (–º–µ–¥–ª–µ–Ω–Ω–µ–µ, –Ω–æ —ç–∫–æ–Ω–æ–º–∏—Ç –ø–∞–º—è—Ç—å)
                            if hasattr(self.pipe, 'enable_sequential_cpu_offload'):
                                self.pipe.enable_sequential_cpu_offload()
                                logger.info("Sequential CPU offload –≤–∫–ª—é—á–µ–Ω (—ç–∫–æ–Ω–æ–º–∏—è –ø–∞–º—è—Ç–∏, –Ω–æ –º–µ–¥–ª–µ–Ω–Ω–µ–µ)")
                            else:
                                self.pipe.enable_model_cpu_offload()
                                logger.info("CPU offload –≤–∫–ª—é—á–µ–Ω (sequential –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è)")
                        else:
                            # –û–±—ã—á–Ω—ã–π CPU offload - –∑–∞–≥—Ä—É–∂–∞–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç–æ–≤ –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω–æ (–±—ã—Å—Ç—Ä–µ–µ)
                            self.pipe.enable_model_cpu_offload()
                            logger.info("CPU offload –≤–∫–ª—é—á–µ–Ω (–æ–±—ã—á–Ω—ã–π —Ä–µ–∂–∏–º - –±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Ç—Ä–µ–±—É–µ—Ç –±–æ–ª—å—à–µ VRAM)")
                        
                        # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –ø–æ—Å–ª–µ –≤–∫–ª—é—á–µ–Ω–∏—è CPU offload
                        if torch.cuda.is_available():
                            torch.cuda.synchronize()
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –≤–∫–ª—é—á–∏—Ç—å CPU offload: {e}. –ó–∞–≥—Ä—É–∂–∞—é –º–æ–¥–µ–ª—å –Ω–∞ GPU.")
                        device_obj = torch.device(self.device)
                        self.pipe.to(device_obj)
                else:
                    device_obj = torch.device(self.device)
                    self.pipe.to(device_obj)
            
                # –®–∞–≥ 6: –§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞
                if progress:
                    progress.update(6, "–§–∏–Ω–∞–ª—å–Ω–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞...")
                if monitor:
                    monitor.update_activity()
                
                # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è –∏ –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏
                if self.device == 'cuda' and torch.cuda.is_available():
                    torch.cuda.synchronize()
                    torch.cuda.empty_cache()
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    memory_free = memory_total - memory_reserved
                    logger.info(f"VRAM –ø–æ—Å–ª–µ –∑–∞–≥—Ä—É–∑–∫–∏: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                    logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB")
                
                # –û—Ç–∫–ª—é—á–µ–Ω–∏–µ progress bar –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
                if hasattr(self.pipe, 'set_progress_bar_config'):
                    self.pipe.set_progress_bar_config(disable=None)
                
                logger.info(f"–ú–æ–¥–µ–ª—å Qwen-Image-Edit-2511 –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {self.device}")
                
                if progress:
                    progress.finish("–ú–æ–¥–µ–ª—å Qwen-Image-Edit-2511 —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
                # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
                if self.device == 'cuda' and torch.cuda.is_available():
                    torch.cuda.empty_cache()
                raise
        except Exception as e:
            logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –º–æ–¥–µ–ª–∏: {e}")
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –≤ —Å–ª—É—á–∞–µ –æ—à–∏–±–∫–∏
            if self.device == 'cuda' and torch.cuda.is_available():
                torch.cuda.empty_cache()
            raise
        finally:
            if monitor:
                monitor.stop()
    
    def edit(
        self,
        images: Union[Image.Image, List[Image.Image]],
        prompt: str,
        negative_prompt: str = " ",
        num_inference_steps: Optional[int] = None,
        guidance_scale: Optional[float] = None,
        true_cfg_scale: Optional[float] = None,
        num_images_per_prompt: int = 1,
        seed: Optional[int] = None,
        save_path: Optional[str] = None
    ) -> Image.Image:
        """
        –†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è(–∏–π) –ø–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–º—É –ø—Ä–æ–º–ø—Ç—É
        
        Args:
            images: –û–¥–Ω–æ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –≤—Ö–æ–¥–Ω—ã—Ö –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            prompt: –¢–µ–∫—Å—Ç–æ–≤–æ–µ –æ–ø–∏—Å–∞–Ω–∏–µ –∂–µ–ª–∞–µ–º–æ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            negative_prompt: –ù–µ–≥–∞—Ç–∏–≤–Ω—ã–π –ø—Ä–æ–º–ø—Ç
            num_inference_steps: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ —à–∞–≥–æ–≤ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞
            guidance_scale: –ú–∞—Å—à—Ç–∞–± guidance
            true_cfg_scale: –ú–∞—Å—à—Ç–∞–± CFG –¥–ª—è Qwen
            num_images_per_prompt: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–∞ –ø—Ä–æ–º–ø—Ç
            seed: –°–ª—É—á–∞–π–Ω–æ–µ –∑–µ—Ä–Ω–æ
            save_path: –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è
        
        Returns:
            PIL.Image: –û—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        """
        if self.pipe is None:
            raise RuntimeError("–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
        
        # –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –æ–¥–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å–ø–∏—Å–æ–∫
        if isinstance(images, Image.Image):
            images = [images]
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∑–Ω–∞—á–µ–Ω–∏–π –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é
        edit_config = self.model_config.get('edit', {})
        num_inference_steps = num_inference_steps or edit_config.get('default_num_inference_steps', 40)
        guidance_scale = guidance_scale if guidance_scale is not None else edit_config.get('default_guidance_scale', 1.0)
        true_cfg_scale = true_cfg_scale if true_cfg_scale is not None else edit_config.get('default_true_cfg_scale', 4.0)
        
        # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ CUDA –ø–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
        if self.device == 'cuda':
            if not torch.cuda.is_available():
                raise RuntimeError("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –Ω–æ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ –∫–∞–∫ 'cuda'")
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞, —á—Ç–æ GPU –Ω–µ –∑–∞–Ω—è—Ç
            try:
                torch.cuda.synchronize()
                logger.info(f"CUDA —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–∞. –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {torch.cuda.get_device_name(0)}")
            except Exception as e:
                logger.warning(f"–ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏–∏ CUDA: {e}")
            
            # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ CUDA –ø–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            torch.cuda.empty_cache()
            
            # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ–π –ø–∞–º—è—Ç–∏
            if torch.cuda.is_available():
                memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_free = memory_total - memory_reserved
                logger.info(f"VRAM –ø–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB")
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ–º
        if seed is not None:
            device_obj = torch.device(self.device)
            generator = torch.Generator(device_obj).manual_seed(seed)
        else:
            generator = None
        
        logger.info(f"–†–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {prompt[:50]}...")
        logger.info(f"–ü–∞—Ä–∞–º–µ—Ç—Ä—ã: —à–∞–≥–æ–≤: {num_inference_steps}, device: {self.device}")
        
        try:
            inputs = {
                "image": images,
                "prompt": prompt,
                "generator": generator,
                "true_cfg_scale": true_cfg_scale,
                "negative_prompt": negative_prompt,
                "num_inference_steps": num_inference_steps,
                "guidance_scale": guidance_scale,
                "num_images_per_prompt": num_images_per_prompt,
            }
            
            # –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–∞–º—è—Ç–∏ –ø–µ—Ä–µ–¥ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ–º
            if self.device == 'cuda' and torch.cuda.is_available():
                memory_before = torch.cuda.memory_allocated(0) / 1024**3
                logger.debug(f"VRAM –ø–µ—Ä–µ–¥ –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–æ–º: {memory_before:.2f}GB")
            
            with torch.inference_mode():
                output = self.pipe(**inputs)
                image = output.images[0]
            
            # –°–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è CUDA –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
            if self.device == 'cuda':
                torch.cuda.synchronize()
                
                # –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–∞–º—è—Ç–∏ –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
                if torch.cuda.is_available():
                    memory_allocated = torch.cuda.memory_allocated(0) / 1024**3
                    memory_reserved = torch.cuda.memory_reserved(0) / 1024**3
                    memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    memory_free = memory_total - memory_reserved
                    usage_percent = (memory_reserved / memory_total) * 100
                    
                    logger.info(f"VRAM –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {memory_allocated:.2f}GB –≤—ã–¥–µ–ª–µ–Ω–æ, {memory_reserved:.2f}GB –∑–∞—Ä–µ–∑–µ—Ä–≤–∏—Ä–æ–≤–∞–Ω–æ")
                    logger.info(f"VRAM –æ–±—â–∞—è: {memory_total:.2f}GB, —Å–≤–æ–±–æ–¥–Ω–æ: {memory_free:.2f}GB, –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ: {usage_percent:.1f}%")
                    
                    # –ü—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏–µ –ø—Ä–∏ –≤—ã—Å–æ–∫–æ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ –ø–∞–º—è—Ç–∏
                    if usage_percent > 95:
                        logger.warning("‚ö†Ô∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –±–ª–∏–∑–∫–æ –∫ –º–∞–∫—Å–∏–º—É–º—É! –†–µ–∫–æ–º–µ–Ω–¥—É–µ—Ç—Å—è:")
                        logger.warning("   - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ enable_cpu_offload: true")
                        logger.warning("   - –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ sequential_offload: true")
                        logger.warning("   - –ó–∞–∫—Ä–æ–π—Ç–µ –¥—Ä—É–≥–∏–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–µ GPU")
                    elif usage_percent > 85:
                        logger.info("‚ÑπÔ∏è –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –≤—ã—Å–æ–∫–æ–µ, –Ω–æ –≤ –¥–æ–ø—É—Å—Ç–∏–º—ã—Ö –ø—Ä–µ–¥–µ–ª–∞—Ö")
                    else:
                        logger.info("‚úÖ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ VRAM –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ")
                
                # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –ø–æ—Å–ª–µ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏—è (–≤–∞–∂–Ω–æ –¥–ª—è CPU offload)
                torch.cuda.empty_cache()
            
            if save_path:
                self._save_image(image, save_path)
            
            logger.info("–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –æ—Ç—Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–æ")
            return image
            
        except RuntimeError as e:
            error_msg = str(e)
            if "CUDA" in error_msg or "cuda" in error_msg:
                logger.error(f"CUDA –æ—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏: {e}")
                if self.device == 'cuda':
                    # –ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è: –æ—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∏ –ø–æ–≤—Ç–æ—Ä–Ω–∞—è —Å–∏–Ω—Ö—Ä–æ–Ω–∏–∑–∞—Ü–∏—è
                    try:
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                        logger.info("–ü–æ–ø—ã—Ç–∫–∞ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏—è CUDA –≤—ã–ø–æ–ª–Ω–µ–Ω–∞")
                    except:
                        pass
            raise
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ —Ä–µ–¥–∞–∫—Ç–∏—Ä–æ–≤–∞–Ω–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è: {e}")
            raise


class ModelFactory:
    """–§–∞–±—Ä–∏–∫–∞ –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–æ–≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    
    MODEL_TYPES = {
        'z-image-turbo': ZImageGenerator,
        'qwen-image-edit': QwenImageEditGenerator,
    }
    
    @staticmethod
    def create_generator(model_type: str, config_path: str = "config.yaml") -> BaseImageGenerator:
        """
        –°–æ–∑–¥–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–≥–æ —Ç–∏–ø–∞
        
        Args:
            model_type: –¢–∏–ø –º–æ–¥–µ–ª–∏ ('z-image-turbo' –∏–ª–∏ 'qwen-image-edit')
            config_path: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        
        Returns:
            BaseImageGenerator: –≠–∫–∑–µ–º–ø–ª—è—Ä –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä–∞
        """
        # –ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
        else:
            raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_path} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        # –ü–æ–ª—É—á–µ–Ω–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –º–æ–¥–µ–ª–∏
        models_config = config.get('models', {})
        if model_type not in models_config:
            raise ValueError(f"–ú–æ–¥–µ–ª—å '{model_type}' –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏. –î–æ—Å—Ç—É–ø–Ω—ã–µ –º–æ–¥–µ–ª–∏: {list(models_config.keys())}")
        
        model_config = models_config[model_type]
        model_class = ModelFactory.MODEL_TYPES.get(model_type)
        
        if model_class is None:
            raise ValueError(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø –º–æ–¥–µ–ª–∏: {model_type}. –î–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–∏–ø—ã: {list(ModelFactory.MODEL_TYPES.keys())}")
        
        return model_class(config, model_config)
    
    @staticmethod
    def get_available_models(config_path: str = "config.yaml") -> List[str]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            return list(config.get('models', {}).keys())
        return []

