# Оптимизация памяти для RTX 4060 (8GB VRAM)

## Проблема
Даже с CPU offload модель не помещается в 8GB VRAM при генерации.

## Реализованные оптимизации

### 1. Sequential CPU Offload
Более агрессивный режим CPU offload:
- Компоненты модели загружаются на GPU **по одному**
- После использования сразу выгружаются обратно в RAM
- Минимальное использование VRAM

### 2. Очистка кэша CUDA
- Очистка перед генерацией
- Очистка после генерации
- Освобождает фрагментированную память

### 3. Уменьшенные размеры
- По умолчанию: **384x384** (можно увеличить до 512/768 если работает)
- Меньше размер = меньше памяти

## Текущая конфигурация

```yaml
device:
  enable_cpu_offload: true  # Sequential CPU offload

generation:
  default_height: 384  # Начните с 384
  default_width: 384
```

## Про AMD видеокарту

**Важно:** PyTorch не может использовать обе видеокарты (NVIDIA + AMD) одновременно для одной модели.

- **NVIDIA RTX 4060** - используется для вычислений (CUDA)
- **AMD Radeon 610M** - не используется (разные API: CUDA vs ROCm)

Для использования AMD нужен PyTorch с поддержкой ROCm, но это:
- Сложная настройка
- Медленнее чем CUDA
- Не все модели поддерживают

**Рекомендация:** Используйте только NVIDIA RTX 4060 с оптимизациями.

## Проверка использования памяти

```bash
nvidia-smi -l 1
```

Во время генерации должно быть:
- **Memory-Usage**: 4-6GB / 8GB (не 8GB!)
- **GPU-Util**: 80-100% во время шагов

## Если все еще ошибка памяти

1. **Закройте другие приложения** использующие GPU
2. **Уменьшите размер до 256x256** в веб-интерфейсе
3. **Уменьшите количество шагов** до 6-7
4. **Перезапустите приложение** для очистки памяти

## Увеличение размера

После успешной генерации 384x384:
1. Попробуйте **512x512** в веб-интерфейсе
2. Если работает - попробуйте **768x768**
3. **1024x1024** скорее всего не поместится

## Что изменилось в коде

1. `enable_sequential_cpu_offload()` вместо `enable_model_cpu_offload()`
2. `torch.cuda.empty_cache()` перед и после генерации
3. Размеры уменьшены до 384x384

**Перезапустите приложение и попробуйте снова!**

